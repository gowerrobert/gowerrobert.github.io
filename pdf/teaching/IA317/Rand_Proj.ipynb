{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB: Random Projections : SOLUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Authors: R. M. Gower\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to\n",
    "- to show that in practice dimension reduction can be used with no loss of accuracy on some problem\n",
    "- code efficient sparse random projections\n",
    "- apply sparse random projections together with knearestneighbors\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- Each paris of students must send their jupyter notebook **before the 24th of November at 21:59** to **gowerrobert@gmail.com**\n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab1_gower_robert_and_bianchi_pascal.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"robert\"\n",
    "ln1 = \"gower\"\n",
    "fn2 = \"pascal\"\n",
    "ln2 = \"bianchi\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"lab1\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throughout the notebook you will find commented boxes like this one\n",
    "\n",
    "### TODO ###   \n",
    "# please implement blabla\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These boxes need to be replaced by code as explained in the boxes.\n",
    "Solutions will online tomorrow. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression    #Logistic Regression\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "def get_data(dataname):\n",
    "    data = load_svmlight_file(dataname)\n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1: \n",
    "\n",
    "Use LogisticRegression classifier of Scikit-learn to experimentally confirm the following corollary proven in class and test random sparse projections\n",
    "\n",
    "### Corollary of Range Space Preserving Theorem  \n",
    "\n",
    "Let \n",
    "$$ X^\\top = [x_1, \\ldots, x_n]\\in \\mathbb{R}^{n\\times d},$$\n",
    "be our data matrix and let\n",
    "$$  X X^\\top= [\\hat{x}_1, \\ldots, \\hat{x}_n]^\\top \\in \\mathbb{R}^{n\\times n}. $$\n",
    "We can find a solution to the following training problem\n",
    "$$ w^* \\in \\min_{w \\in \\mathbb{R}^d}  \\frac{1}{n}\\sum_{i=1}^n \\ell_i(\\langle x_i,w \\rangle) \\hspace{3cm} (I)$$\n",
    "by instead solving \n",
    "$$ \\hat{w}^* \\in \\min_{w \\in \\mathbb{R}^n}  \\frac{1}{n}\\sum_{i=1}^n \\ell_i(\\langle \\hat{x}_i,w \\rangle) \\hspace{3cm} (II)$$\n",
    "and  $ X ^\\top \\hat{w}^*$ is a solution to $(I)$\n",
    "\n",
    "**NOTE:** The matrix $X$ is transposed with respect to the data matrix defined in class and in the lectures ! Be careful with dimnensions!\n",
    "### End Corollary\n",
    "\n",
    "1) [2pts] Show that by setting the regularization parameter close to zero (C = 10^9) in LogisticRegression, the score obtained by training using $X$ and $XX^\\top$ is the same\n",
    "  \n",
    "2) [2pts] Compute a solution $w^*_1$  by directly solving (I).  Compare this $w^*_1$ to the recovered solution $X \\hat{w}^*$. Are they the same? Justify based on Corollary.\n",
    "\n",
    "3) [6pts] Using a random generated gaussian matrix $W \\in\\mathbb{R}^{d\\times r}$ , project the data matrix $X \\rightarrow XW$. Test for different values of r and\n",
    "apply logistic regression to the resulting projected matrix. Can you explain what you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 data points and 2000 features\n"
     ]
    }
   ],
   "source": [
    "# download the colon-cancer data set from \n",
    "# https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/colon-cancer.bz2\n",
    "# Unpack and place in the same folder as this python notebook\n",
    "dataname = \"colon-cancer\"  \n",
    "X, y = get_data(dataname)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "n, d = X_train.shape\n",
    "print('{n} data points and {d} features'.format(n = n,d =d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 1.000\n",
      "Accuracy on the test set: 0.619\n"
     ]
    }
   ],
   "source": [
    "C0 = 10**9 # almost no regularization, since this is the inverse of the regularization parameter, i.e, C = 1/lambda\n",
    "log_reg = LogisticRegression(C = C0) # , multi_class = \"multinomial\"\n",
    "log_reg.fit(X_train, y_train)\n",
    "print('Accuracy on the training set: {:.3f}'.format(log_reg.score(X_train,y_train)))\n",
    "print('Accuracy on the test set: {:.3f}'.format(log_reg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO ###   \n",
    "# 2)  Compute a solution $w^*_1$  by directly solving (I).  \n",
    "# Compare this $w^*_1$ to the recovered solution $X^\\top \\hat{w}^*$.\n",
    "# Are they the same? Justify based on Corollary.\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project dimension    1 gives: (train, test) =  (0.0000, 0.0000)\n",
      "project dimension  201 gives: (train, test) =  (0.0000, 0.0000)\n",
      "project dimension  401 gives: (train, test) =  (0.0000, 0.0000)\n",
      "project dimension  601 gives: (train, test) =  (0.0000, 0.0000)\n",
      "project dimension  801 gives: (train, test) =  (0.0000, 0.0000)\n",
      "project dimension 1001 gives: (train, test) =  (0.0000, 0.0000)\n",
      "project dimension 1201 gives: (train, test) =  (0.0000, 0.0000)\n",
      "project dimension 1401 gives: (train, test) =  (0.0000, 0.0000)\n",
      "project dimension 1601 gives: (train, test) =  (0.0000, 0.0000)\n",
      "project dimension 1801 gives: (train, test) =  (0.0000, 0.0000)\n",
      "Best score was for r =    1 with: (train, test) =  (0.0000, 0.0000)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmYFOW5/vHvIyBEISCL64ADCS7jMDPAACKiEkEQEQWU4E8jxAXXEE+iR/JTkaCeA0cjSoy7AipBAirg0SgKGFEjMqBGGVEWMWCQVQiDIss854+qaZphlh6onqbh/lxXX1S9/XbV0zVD31NVXW+ZuyMiIrKvDkl1ASIicmBQoIiISCQUKCIiEgkFioiIREKBIiIikVCgiIhIJBQoIiISCQWKiIhEQoEiIiKRqJnqAqpT48aNPTMzM9VliIiklfnz569z9yaV9TuoAiUzM5OCgoJUlyEiklbM7KtE+umQl4iIREKBIiIikVCgiIhIJA6qcygi1WX79u2sXLmSrVu3proUkYTVqVOHjIwMatWqtVevV6CIJMHKlSupV68emZmZmFmqyxGplLuzfv16Vq5cSfPmzfdqGTrkJZIEW7dupVGjRgoTSRtmRqNGjfZpr1qBIpIkChNJN/v6O6tAERGRSChQRA5gU6dOxcxYtGhRqkuJ1JgxYzj55JO59NJLd2v/6KOPePXVV2Pzw4cP57777tvr9TzwwAN89913VX7dsGHDePPNNyvsM336dEaOHLm3pe210tsoSgoUkQPYxIkTOf3005k4cWJS17Nz586kLr+0hx9+mDfeeIMJEybs1h71h2VFgVLRex4xYgRdu3atcNm9e/dm6NCh+1Tf3lCgiEiVFRUV8c477/DUU0/x/PPP7/bcqFGjaNWqFbm5ubEPtSVLltC1a1dyc3Np06YNS5cu5a233qJXr16x1914442MGzcOCIYyuvXWW2nTpg2TJ0/miSeeoF27duTm5tKvX7/YB/Hq1avp06cPubm55Obm8t577zFs2DAeeOCB2HJvu+02HnzwwT3ew/333092djbZ2dmx/tdeey3Lli3j3HPPZfTo0bG+27ZtY9iwYUyaNIm8vDwmTZoEQGFhIWeddRYtWrRgzJgxsf7PPfcc7du3Jy8vj2uuuWaPgBgzZgz/+te/6NKlC126dAGgbt26/Pa3vyU3N5e///3vjBgxgnbt2pGdnc3gwYNxdwAGDRrElClTYtvpzjvvpE2bNrRq1Sq2tzhu3DhuvPHGWP8hQ4Zw2mmn0aJFi9hri4uLuf766znppJPo1q0bPXv2jD1XutasrCxycnIYMGAAAFu2bOGKK66gffv2tG7dmmnTppW7jaKirw2LJNnvX15I4b/+Hekys479MXeef0qFfaZNm0aPHj044YQTaNSoEfPnz6dt27b89a9/Zdq0acydO5fDDjuMDRs2AHDppZcydOhQ+vTpw9atWykuLmbFihUVrqNRo0YsWLAAgPXr13P11VcDcPvtt/PUU0/xq1/9iiFDhnDmmWfy0ksvsXPnToqKijj22GPp27cvN910E8XFxTz//PN88MEHuy17/vz5jB07lrlz5+LudOjQgTPPPJNHH32U1157jdmzZ9O4ceNY/0MPPZQRI0ZQUFDAQw89BASHvBYtWsTs2bPZvHkzJ554Itdddx1Llixh0qRJvPvuu9SqVYvrr7+eCRMmcPnll8eWN2TIEO6///7d1rNlyxY6dOjAH/7wh+DnkJXFsGHDAPjFL37B//7v/3L++efvsZ0aN27MggULePjhh7nvvvt48skn9+izatUq3nnnHRYtWkTv3r256KKLePHFF1m+fDmFhYWsWbOGk08+mSuuuGKP144cOZIvv/yS2rVrs3HjRgDuuecefvazn/H000+zceNG2rdvT9euXffYRlHSHorIAWrixImxv1YHDBgQO+z15ptv8stf/pLDDjsMgIYNG7J582a+/vpr+vTpAwQXuJU8X5Gf//znselPP/2Uzp0706pVKyZMmMDChQsBmDVrFtdddx0ANWrUoH79+mRmZtKoUSM+/PBDZsyYQevWrWnUqNFuy37nnXfo06cPhx9+OHXr1qVv377MmTOnytvhvPPOo3bt2jRu3JgjjzyS1atXM3PmTObPn0+7du3Iy8tj5syZLFu2rNJl1ahRg379+sXmZ8+eTYcOHWjVqhWzZs2KvefS+vbtC0Dbtm1Zvnx5mX0uvPBCDjnkELKysli9ejUQbIOLL76YQw45hKOPPjq2p1RaTk4Ol156Kc899xw1awb7CTNmzGDkyJHk5eVx1llnsXXrVv75z39W+h73hfZQRJKssj2JZNiwYQOzZs3ik08+wczYuXMnZsa9995bpeXUrFmT4uLi2HzpaxQOP/zw2PSgQYOYOnUqubm5jBs3jrfeeqvCZV911VWMGzeOb775psy/uqNSu3bt2HSNGjXYsWMH7s7AgQP57//+7yotq06dOtSoUQMItsX1119PQUEBTZs2Zfjw4eVew1FSQ8n6K6uz5NBZol555RXefvttXn75Ze655x4++eQT3J0XXniBE088cbe+c+fOrdKyq0J7KCIHoClTpvCLX/yCr776iuXLl7NixQqaN2/OnDlz6NatG2PHjo2d49iwYQP16tUjIyODqVOnAvDDDz/w3Xffcfzxx1NYWMgPP/zAxo0bmTlzZrnr3Lx5M8cccwzbt2/f7WT52WefzSOPPAIEJ7I3bdoEQJ8+fXjttdeYN28e3bt332N5nTt3ZurUqXz33Xds2bKFl156ic6dO1f4vuvVq8fmzZsr3T5nn302U6ZMYc2aNbFt8NVXe47QXtHySsKjcePGFBUVlXluY1916tSJF154geLiYlavXl1mSJccmuzSpQujRo1i06ZNFBUV0b17d/74xz/GwunDDz+s9D3tKwWKyAFo4sSJscNXJfr168fEiRPp0aMHvXv3Jj8/n7y8vNjXap999lnGjBlDTk4Op512Gt988w1Nmzalf//+ZGdn079/f1q3bl3uOu+66y46dOhAp06dOOmkk2LtDz74ILNnz6ZVq1a0bduWwsJCIDjn0aVLF/r37x/7qz9emzZtGDRoEO3bt6dDhw5cddVVFa4foEuXLhQWFlZ6wjkrK4u7776bc845h5ycHLp168aqVav26Dd48GB69OhR5qGmBg0acPXVV5OdnU337t1p165dhbXtjX79+pGRkUFWVhaXXXYZbdq0oX79+rv12blzJ5dddhmtWrWidevWDBkyhAYNGnDHHXewfft2cnJyOOWUU7jjjjuAxLfR3rCq7lqls/z8fNcNtqQ6fPbZZ5x88smpLmO/VlxcHPuGWMuWLVNdzn6rqKiIunXrsn79etq3b8+7777L0UcfnbT1lfW7a2bz3T2/stfqHIqIVLvCwkJ69epFnz59FCaV6NWrFxs3bmTbtm3ccccdSQ2TfaVAEZFql5WVldC3qoRKv9ywP9E5FBERiYQCRUREIqFAERGRSChQREQkEgoUkQOYhq9PzfD1EGz7kmtu9sXy5cv585//vM/LqQ4pDRQz62Fmn5vZEjPbYxxnM6ttZpPC5+eaWWap55uZWZGZ3VxdNYukEw1fv28UKFWTskAxsxrAn4BzgSzgEjPLKtXtSuBbd/8pMBoYVer5+4G/JrtWkXSk4eujH75+xowZdOzYkTZt2nDxxRdTVFQEwNChQ2PDx99888289957TJ8+nVtuuYW8vDyWLl2627InT55MdnY2ubm5nHHGGUAQyrfccgvt2rUjJyeHxx57LLbsOXPmkJeXt9v73S+5e0oeQEfg9bj53wG/K9XndaBjOF0TWMeuq/svBO4FhgM3J7LOtm3bukh1KCws3DXz6q3uT/eM9vHqrZXW8Nxzz/kVV1zh7u4dO3b0goKCoJxXX/WOHTv6li1b3N19/fr17u7evn17f/HFF93d/fvvv/ctW7b47Nmz/bzzzost84YbbvCxY8e6u/vxxx/vo0aNij23bt262PRtt93mY8aMcXf3/v37++jRo93dfceOHb5x40b/8ssvvXXr1u7uvnPnTm/RosVur3d3Lygo8OzsbC8qKvLNmzd7VlaWL1iwILbutWvX7vGex44d6zfccENs/s477/SOHTv61q1bfe3atd6wYUPftm2bFxYWeq9evXzbtm3u7n7dddf5+PHj91he/HrWrl3rnTt39qKiInd3HzlypP/+97/3devW+QknnODFxcXu7v7tt9+6u/vAgQN98uTJeyzT3T07O9tXrly5W//HHnvM77rrLnd337p1q7dt29aXLVu2x88g2Xb73Q0BBZ7AZ2wqL2w8Doi/2cJKoEN5fdx9h5ltAhqZ2VbgVqAboMNdImWYOHEiv/71r4Fdw9e3bds24eHrE1F6+Prbb7+djRs3xgYnhGD4+meeeQbYNXx9/fr1Y8PXr169utLh64HY8PWVjedVWsnw9bVr1y5z+HqA77//niOPPLLC5bz//vsUFhbSqVMnINgj6tixI/Xr16dOnTpceeWV9OrVa7c9uvJ06tSJQYMG0b9//9jQ9jNmzOAf//hHbJDJTZs2sXjxYg499NAqvd9UStcr5YcDo929yMwq7Ghmg4HBAM2aNUt+ZSKlnVv99w3X8PW7RDV8vbvTrVu3Ms9HffDBB8ycOZMpU6bw0EMPMWvWrAqX9eijjzJ37lxeeeUV2rZty/z583F3/vjHP+4x8rKulE/M10DTuPmMsK3MPmZWE6gPrCfYk/kfM1sO3AT8fzO7sayVuPvj7p7v7vlNmjSJ9h2I7Kc0fH3F9mb4+lNPPZV3332XJUuWAMHdG7/44guKiorYtGkTPXv2ZPTo0Xz88ceV1rJ06VI6dOjAiBEjaNKkCStWrKB79+488sgjbN++HYAvvviCLVu2JHW4+ailMlDmAS3NrLmZHQoMAKaX6jMdGBhOXwTMCg/pdXb3THfPBB4A/svdo7+fpUia0vD10Q9f36RJE8aNG8cll1xCTk4OHTt2ZNGiRWzevJlevXqRk5PD6aefzv333w8EhxnvvfdeWrduvcdJ+VtuuYVWrVqRnZ3NaaedRm5uLldddRVZWVm0adOG7OxsrrnmGnbs2EFOTg41atQgNzd3vz8pn9Lh682sJ0Eg1ACedvd7zGwEwQmg6WZWB3gWaA1sAAa4+7JSyxgOFLl7pV821/D1Ul00fH3lNHz9/ilth69391eBV0u1DYub3gpcXMkyhielOBFJGg1ff2BK15PyIpLGNHz9gUlDr4gkSSoPJ4vsjX39nVWgiCRBnTp1WL9+vUJF0oa7s379+oSvQSqLDnmJJEFGRgYrV65k7dq1qS5FJGF16tQhIyNjr1+vQBFJglq1atG8efNUlyFSrXTIS0REIqFAERGRSChQREQkEgoUERGJhAJFREQioUAREZFIKFBERCQSChQREYmEAkVERCKhQBERkUgoUEREJBIKFBERiYQCRUREIqFAERGRSChQREQkEgoUERGJhAJFREQioUAREZFIKFBERCQSChQREYmEAkVERCKhQBERkUgoUEREJBIKFBERiYQCRUREIqFAERGRSKQ0UMysh5l9bmZLzGxoGc/XNrNJ4fNzzSwzbO9mZvPN7JPw359Vd+0iIrK7lAWKmdUA/gScC2QBl5hZVqluVwLfuvtPgdHAqLB9HXC+u7cCBgLPVk/VIiJSnlTuobQHlrj7MnffBjwPXFCqzwXA+HB6CnC2mZm7f+ju/wrbFwI/MrPa1VK1iIiUKZWBchywIm5+ZdhWZh933wFsAhqV6tMPWODuPySpThERSUDNVBewL8zsFILDYOdU0GcwMBigWbNm1VSZiMjBJ5V7KF8DTePmM8K2MvuYWU2gPrA+nM8AXgIud/el5a3E3R9393x3z2/SpEmE5YuISLxUBso8oKWZNTezQ4EBwPRSfaYTnHQHuAiY5e5uZg2AV4Ch7v5utVUsIiLlSlmghOdEbgReBz4D/uLuC81shJn1Drs9BTQysyXAb4CSrxbfCPwUGGZmH4WPI6v5LYiISBxz91TXUG3y8/O9oKAg1WWIiKQVM5vv7vmV9dOV8iIiEgkFioiIREKBIiIikVCgiIhIJBQoIiISCQWKiIhEQoEiIiKRUKCIiEgkFCgiIhIJBYqIiERCgSIiIpFQoIiISCQUKCIiEolKA8XMfmVmR1RHMSIikr4S2UM5CphnZn8xsx5mZskuSkRE0k+lgeLutwMtCW52NQhYbGb/ZWY/SXJtIiKSRhI6h+LBXbi+CR87gCOAKWb2P0msTURE0kjNyjqY2a+By4F1wJPALe6+3cwOARYD/5ncEkVEJB1UGihAQ6Cvu38V3+juxWbWKzlliYhIuknkkNdfgQ0lM2b2YzPrAODunyWrMBERSS+JBMojQFHcfFHYJiIiEpNIoFh4Uh4IDnWR2KEyERE5iCQSKMvMbIiZ1QofvwaWJbswERFJL4kEyrXAacDXwEqgAzA4mUWJiEj6qfTQlbuvAQZUQy0iIpLGErkOpQ5wJXAKUKek3d2vSGJdIiKSZhI55PUscDTQHfgbkAFsTmZRIiKSfhIJlJ+6+x3AFncfD5xHcB5FREQkJpFA2R7+u9HMsoH6wJHJK0lERNJRIteTPB7eD+V2YDpQF7gjqVWJiEjaqXAPJRwA8t/u/q27v+3uLdz9SHd/LIqVh/dX+dzMlpjZ0DKer21mk8Ln55pZZtxzvwvbPzez7lHUIyIie6/CQAmvik/KaMJmVgP4E3AukAVcYmZZpbpdCXzr7j8FRgOjwtdmEXyV+RSgB/BwuDwREUmRRM6hvGlmN5tZUzNrWPKIYN3tgSXuvszdtwHPAxeU6nMBMD6cngKcHd4x8gLgeXf/wd2/BJaEyxMRkRRJ5BzKz8N/b4hrc6DFPq77OGBF3HzJVfhl9nH3HWa2CWgUtr9f6rXH7WM95Xr/4aupt1EDK4tIetrc4GROvf6JpK8nkSvlmye9iiQys8GEQ8U0a9YsxdWIiBy4ErlS/vKy2t39mX1c99dA07j5jLCtrD4rzawmwVeW1yf42pI6HwceB8jPz/ey+lSmOpJdRCTdJXIOpV3cozMwHOgdwbrnAS3NrLmZHUpwkn16qT7TgYHh9EXArHAo/enAgPBbYM2BlsAHEdQkIiJ7KZFDXr+KnzezBgQn0PdJeE7kRuB1oAbwtLsvNLMRQIG7TweeAp41syUEd40cEL52oZn9BSgEdgA3uPvOfa1JRET2nsXdOyuxF5jVAj519xOTU1Ly5Ofne0FBQarLEBFJK2Y2393zK+uXyDmUlwm+1QXBIbIs4C/7Vp6IiBxoEvna8H1x0zuAr9x9ZZLqERGRNJVIoPwTWOXuWwHM7Edmlunuy5NamYiIpJVEvuU1GSiOm98ZtomIiMQkEig1w6FRAAinD01eSSIiko4SCZS1Zha77sTMLgDWJa8kERFJR4mcQ7kWmGBmD4XzK4Eyr54XEZGDVyIXNi4FTjWzuuF8UdKrEhGRtFPpIS8z+y8za+DuRe5eZGZHmNnd1VGciIikj0TOoZzr7htLZtz9W6Bn8koSEZF0lEig1DCz2iUzZvYjoHYF/UVE5CCUyEn5CcBMMxsLGDCIXXdRFBERARI7KT/KzD4GuhKM6fU6cHyyCxMRkfSSyCEvgNUEYXIx8DNA98MVEZHdlLuHYmYnAJeEj3XAJILh7rtUU20iIpJGKjrktQiYA/Ry9yUAZvYf1VKViIiknYoOefUFVgGzzewJMzub4KS8iIjIHsoNFHef6u4DgJOA2cBNwJFm9oiZnVNdBYqISHqo9KS8u29x9z+7+/lABvAhcGvSKxMRkbSS6Le8gOAqeXd/3N3PTlZBIiKSnqoUKCIiIuVRoIiISCQUKCIiEgkFioiIREKBIiIikVCgiIhIJBQoIiISCQWKiIhEQoEiIiKRUKCIiEgkFCgiIhKJlASKmTU0szfMbHH47xHl9BsY9llsZgPDtsPM7BUzW2RmC81sZPVWLyIiZUnVHspQYKa7twRmhvO7MbOGwJ1AB6A9cGdc8Nzn7icBrYFOZnZu9ZQtIiLlSVWgXACMD6fHAxeW0ac78Ia7b3D3b4E3gB7u/p27zwZw923AAoJh9UVEJIVSFShHufuqcPob4Kgy+hwHrIibXxm2xZhZA+B8gr0cERFJoYruKb9PzOxN4Ogynrotfsbd3cx8L5ZfE5gIjHH3ZRX0GwwMBmjWrFlVVyMiIglKWqC4e9fynjOz1WZ2jLuvMrNjgDVldPsaOCtuPgN4K27+cWCxuz9QSR2Ph33Jz8+vcnCJiEhiUnXIazowMJweCEwro8/rwDlmdkR4Mv6csA0zuxuoT3CfexER2Q+kKlBGAt3MbDHQNZzHzPLN7EkAd98A3AXMCx8j3H2DmWUQHDbLAhaY2UdmdlUq3oSIiOxi7gfPUaD8/HwvKChIdRkiImnFzOa7e35l/XSlvIiIREKBIiIikVCgiIhIJBQoIiISCQWKiIhEQoEiIiKRUKCIiEgkFCgiIhIJBYqIiERCgSIiIpFQoIiISCQUKCIiEgkFioiIREKBIiIikVCgiIhIJBQoIiISCQWKiIhEQoEiIiKRUKCIiEgkFCgiIhIJBYqIiERCgSIiIpFQoIiISCQUKCIiEgkFioiIREKBIiIikVCgiIhIJBQoIiISCQWKiIhEQoEiIiKRSEmgmFlDM3vDzBaH/x5RTr+BYZ/FZjawjOenm9mnya9YREQqk6o9lKHATHdvCcwM53djZg2BO4EOQHvgzvjgMbO+QFH1lCsiIpVJVaBcAIwPp8cDF5bRpzvwhrtvcPdvgTeAHgBmVhf4DXB3NdQqIiIJSFWgHOXuq8Lpb4CjyuhzHLAibn5l2AZwF/AH4LukVSgiIlVSM1kLNrM3gaPLeOq2+Bl3dzPzKiw3D/iJu/+HmWUm0H8wMBigWbNmia5GRESqKGmB4u5dy3vOzFab2THuvsrMjgHWlNHta+CsuPkM4C2gI5BvZssJ6j/SzN5y97Mog7s/DjwOkJ+fn3BwiYhI1aTqkNd0oORbWwOBaWX0eR04x8yOCE/GnwO87u6PuPux7p4JnA58UV6YiIhI9UlVoIwEupnZYqBrOI+Z5ZvZkwDuvoHgXMm88DEibBMRkf2QuR88R4Hy8/O9oKAg1WWIiKQVM5vv7vmV9dOV8iIiEgkFioiIREKBIiIikVCgiIhIJBQoIiISCQWKiIhEQoEiIiKRUKCIiEgkFCgiIhIJBYqIiERCgSIiIpFQoIiISCQUKCIiEgkFioiIREKBIiIikVCgiIhIJBQoIiISCQWKiIhEQoEiIiKRUKCIiEgkFCgiIhIJBYqIiERCgSIiIpFQoIiISCQUKCIiEglz91TXUG3MbC3w1V6+vDGwLsJykkV1Ri9dalWd0UqXOiH5tR7v7k0q63RQBcq+MLMCd89PdR2VUZ3RS5daVWe00qVO2H9q1SEvERGJhAJFREQioUBJ3OOpLiBBqjN66VKr6oxWutQJ+0mtOociIiKR0B6KiIhEQoFSCTPrYWafm9kSMxua4lqamtlsMys0s4Vm9uuwfbiZfW1mH4WPnnGv+V1Y++dm1r2a611uZp+ENRWEbQ3N7A0zWxz+e0TYbmY2Jqz1H2bWpppqPDFuu31kZv82s5v2h21qZk+b2Roz+zSurcrbz8wGhv0Xm9nAaqz1XjNbFNbzkpk1CNszzez7uG37aNxr2oa/M0vC92PVUGeVf9bJ/lwop85JcTUuN7OPwvaUbc89uLse5TyAGsBSoAVwKPAxkJXCeo4B2oTT9YAvgCxgOHBzGf2zwpprA83D91KjGutdDjQu1fY/wNBweigwKpzuCfwVMOBUYG6Kft7fAMfvD9sUOANoA3y6t9sPaAgsC/89Ipw+oppqPQeoGU6Piqs1M75fqeV8ENZv4fs5txrqrNLPujo+F8qqs9TzfwCGpXp7ln5oD6Vi7YEl7r7M3bcBzwMXpKoYd1/l7gvC6c3AZ8BxFbzkAuB5d//B3b8ElhC8p1S6ABgfTo8HLoxrf8YD7wMNzOyYaq7tbGCpu1d08Wu1bVN3fxvYUMb6q7L9ugNvuPsGd/8WeAPoUR21uvsMd98Rzr4PZFS0jLDeH7v7+x58Gj7DrveXtDorUN7POumfCxXVGe5l9AcmVrSM6tiepSlQKnYcsCJufiUVf4BXGzPLBFoDc8OmG8NDC0+XHAYh9fU7MMPM5pvZ4LDtKHdfFU5/AxwVTqe6VoAB7P6fdH/cplXdfqmut8QVBH8hl2huZh+a2d/MrHPYdhxBfSWqs9aq/KxTvU07A6vdfXFc236xPRUoacjM6gIvADe5+7+BR4CfAHnAKoLd4f3B6e7eBjgXuMHMzoh/Mvyrab/4mqGZHQr0BiaHTfvrNo3Zn7ZfRczsNmAHMCFsWgU0c/fWwG+AP5vZj1NVH2nwsy7lEnb/w2e/2Z4KlIp9DTSNm88I21LGzGoRhMkEd38RwN1Xu/tOdy8GnmDXIZiU1u/uX4f/rgFeCutaXXIoK/x3zf5QK0HoLXD31bD/blOqvv1SWq+ZDQJ6AZeGAUh4CGl9OD2f4HzECWFd8YfFqqXWvfhZp2ybmllNoC8wqaRtf9qeCpSKzQNamlnz8C/YAcD0VBUTHjt9CvjM3e+Pa48/19AHKPlmyHRggJnVNrPmQEuCk3TVUevhZlavZJrgBO2nYU0l3zQaCEyLq/Xy8NtKpwKb4g7tVIfd/urbH7dp3Pqrsv1eB84xsyPCQznnhG1JZ2Y9gP8Eerv7d3HtTcysRjjdgmAbLgvr/beZnRr+rl8e9/6SWWdVf9ap/FzoCixy99ihrP1qeybzjP+B8CD49swXBKl/W4prOZ3gEMc/gI/CR0/gWeCTsH06cEzca24La/+cJH/Do1StLQi+/fIxsLBk2wGNgJnAYuBNoGHYbsCfwlo/AfKrsdbDgfVA/bi2lG9TgoBbBWwnOP595d5sP4LzF0vCxy+rsdYlBOcaSn5XHw379gt/Jz4CFgDnxy0nn+ADfSnwEOHF10mus8o/62R/LpRVZ9g+Dri2VN+Ubc/SD10pLyIikdAhLxERiYQCRUREIqFAERGRSChQREQkEgoUERHSrQS6AAAES0lEQVSJhAJF0paZ7QxHV/3UzCab2WFVfP2rFo6AW8XXnWVmp+3F65abWeNy2j8JH4VmdreZ1QmfO9bMplR1XVHY2+0jBy8FiqSz7909z92zgW3AtfFPhhf5lfs77u493X3jXqz3LKDKgVKJLu7eiuAq7RbAYwDu/i93vyjidSVkH7aPHKQUKHKgmAP8NLw3xOdm9gzBBV1NzeyS8K//T81sVMkL4vcYzOwyM/sg3ON5LO7K4x5mtsDMPjazmeGgnNcC/xH27RxeqfyCmc0LH53C1zYysxkW3LvmSYKLDyvk7kXh8i+04N4nmRbeE8PMBpnZVAvug7LczG40s9+EgwK+b2YNw34/MbPXLBiUc46ZnRS2j7PgnhjvmdkyM7sobD/GzN6O29vrXMb2+U343KdmdlPYlmlmn5nZE+F7nGFmP9rXH6SksWReNamHHsl8AEXhvzUJhpS4juDeEMXAqeFzxwL/BJqE/WYBF4bPLQcaAycDLwO1wvaHCYapaEJwpXfzsL3kqvThxN0/A/gzwUCYAM0IhsYBGMOue1acRzDKQeMy3sfy0u0EVz13IO5eF8AggqvP64W1bSK8ahoYTTBYKARX0rcMpzsAs8LpcQSDXx5CcK+PJWH7b9k1kkENoF6p7dOW4Eryw4G6BFdltw5r2wHkhf3/AlyW6t8LPVL3qFle0IikgR9ZeNc6gj2UpwgC5CsP7gkC0A54y93XApjZBIKbF02NW87ZBB+a84Ihj/gRwaCLpwJve3AvDNy9vPtodAWybNfN8H5swYjQZxAM5Ie7v2Jm31bhvZW3NzPbg3vhbDazTQRBCMEHfk643tOAyXH11I57/VQPBkEsNLOSoe/nAU9bMPDoVHf/iN2dDrzk7lsAzOxFgiHUpwNfxvWfTxAycpBSoEg6+97d8+Ibwg/RLVVcjgHj3f13pZZ1foKvP4Rgj2hrGbVUmQWDamYSjBVVv9TTP8RNF8fNFxP8fz4E2Fh6u5TzeoPgZk4W3FrgPGCcmd3v7s8kWG788nYShLEcpHQORQ50HwBnmlnj8LzIJcDfSvWZCVxkZkdC7L7txxPcZfCMcKRZSs5RAJsJDjuVmAH8qmTGzEo+zN8G/l/Ydi7BLXgrFO5hPEywp1CVPRoAPLg/zpdmdnG4PDOz3ErWeTzBDZueAJ4kuPVsvDkE53QOs2Dk6D5hm8huFChyQPNgCO+hwGyCkY/nu/u03bt4IXA7wd0l/0Fwm9xjwsNkg4EXzexjdt2D4mWgT8lJeWAIkG/BHf8K2fVts98TBNJCgkNf/6yg1NnhyfcPwn7X7MPbvhS4Mqx5IZXfnvYs4GMz+xD4OfBg/JMe3HZ6XFjbXOBJd/9wH+qTA5RGG5aDUri3sgY42t23p7oekQOB9lDkYLWQ4C9thYlIRLSHIiIikdAeioiIREKBIiIikVCgiIhIJBQoIiISCQWKiIhEQoEiIiKR+D8kywmvB3Q7GAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### TODO ### \n",
    "## Choose a range of different projected dimensions to test\n",
    "## Suggested range of projected dimensions:\n",
    "upperbnd = d\n",
    "project_dimensions = range(1,upperbnd,int((upperbnd )/10))\n",
    "#############\n",
    "test_accuracy = []\n",
    "training_accuracy = []\n",
    "s = 20\n",
    "for r in project_dimensions: \n",
    "    ### TODO ###   \n",
    "    # 3)    project the data matrix $X \\rightarrow XW$ using Gaussian and fit, \n",
    "    #      transform and score using Logstic Regression\n",
    "    # trainscore =  ??\n",
    "    trainscore=0 # Change this!\n",
    "    training_accuracy.append(trainscore)\n",
    "    # testscore = ??\n",
    "    testscore=0# Change this!\n",
    "    test_accuracy.append(testscore)\n",
    "    #############\n",
    "    print (\"project dimension %4d gives: (train, test) =  (%.4f, %.4f)\" % (r, trainscore,testscore))\n",
    "# coef_recover= log_regt.coef_.dot(X_train.transpose())\n",
    "\n",
    "plt.plot(project_dimensions,training_accuracy, label='Accuracy of the training set')\n",
    "plt.plot(project_dimensions,test_accuracy, label='Accuracy of the test set')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Projected Dimension')\n",
    "plt.legend()\n",
    "index_max = np.argmax(test_accuracy)\n",
    "print(\"Best score was for r =%5d with: (train, test) =  (%.4f, %.4f)\"% (project_dimensions[index_max], training_accuracy[index_max],test_accuracy[index_max]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "In class we used \n",
    "$$ X \\rightarrow W^\\top X $$\n",
    "But here it is instead\n",
    "$$ X \\rightarrow X W $$\n",
    "\n",
    "\n",
    "\n",
    "Now we will train a model using a large text based data set. For this you need to code the following random transform $W \\in \\mathbb{R}^{d \\times r}$ that takes a sparsity parameter $s$ as an input and outputs a matrix\n",
    "\n",
    "$$ W_{ij}  = \\sqrt{\\frac{s}{r}}\n",
    "\\begin{cases}\n",
    "1 \\quad & \\mbox{with probability }\\frac{1}{2s} \\\\\n",
    "0 \\quad & \\mbox{with probability }1-\\frac{1}{s} \\\\\n",
    "-1 \\quad & \\mbox{with probability }\\frac{1}{2s} \n",
    "\\end{cases}$$\n",
    "\n",
    "* Code a function Generate_Sparse_Transform$(s,r,d)$ that takes an input  \n",
    "   * sparsity parameter  $s$\n",
    "   * input dimension $d \\in \\mathbb{N}$\n",
    "   * lower dimensional projected dimension $r \\in \\mathbb{N}$\n",
    "and gives as outputs the matrix $W$ stored in an efficient sparse format such as the CSC formart (see scipy.sparse.csc_matrix)\n",
    "\n",
    "* Code a function Apply_Sparse_Transform$(R,X)$ that takes an input\n",
    "   * the random transform $W$ as generated by  Generate\\_sparse\\_transform$(s,r,d)$\n",
    "   * a given data matrix $X \\in \\mathbb{R}^{n\\times d}$\n",
    "the output will be $XW$.\n",
    "\n",
    "\n",
    "*Note* if you have not implemented this efficiently, you will probably run out of memory!  \n",
    "\n",
    "\n",
    "We will test if random projections are able to preserve pairwise distances by applying K-Neighrest Neighbors to projected data.\n",
    "\n",
    "1) [6pts] Code the above two functions (their stub is provided below)\n",
    "\n",
    "\n",
    "2) [1pt] Load the data set X. Then fit, transform and score sklearn's KNeighborsClassifier on this data, where \n",
    "$X = $ {anthracyclineTaxaneChemotherapy, sector.scale}. Below you will find how to load this data.\n",
    "\n",
    "3) [3pts] Repeat the previous step, but first randomly project that data using X -> XW. Repeat this test for different values of the sparsity parameter s and projected dimension parameter r. What can you conclude? \n",
    "\n",
    "*Hint* As a rule of thumb $s = \\sqrt{r}$, $s= \\log(r)$ or simply $s=20$ often works well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.utils.extmath import safe_sparse_dot   ## <-- I recommend using this function\n",
    "from sklearn.utils.random import sample_without_replacement  ## <-- I recommend using this function\n",
    "\n",
    "def GenerateSparseTransform(s,r,d):\n",
    "    ### TODO ### \n",
    "    ##  Implement this function. Make sure that W is a sparse matrix!\n",
    "    #############\n",
    "    W = 0\n",
    "    return W\n",
    "\n",
    "def ApplySparseTransform(W,X_):\n",
    "#     W   : Sparse randomly generated matrix of size d by r\n",
    "#     X_  : Data matrix to be compressed, of size n by d\n",
    "#    NOTE: The dimensions of W and X_ are such that the product X_*W is defined (which is different that what we used in class)\n",
    "    ### TODO ### \n",
    "    ##  Implement this function. Make sure that W is a sparse matrix!\n",
    "    #############\n",
    "    Xtransformed =0\n",
    "    return Xtransformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and test easier data set *anthracyclineTaxaneChemotherapy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "chemo = fetch_openml(name='anthracyclineTaxaneChemotherapy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = chemo.data\n",
    "y = chemo.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if fetch_openml fails  \n",
    "**ALTERNATIVE HACK for loading the data**\n",
    "  \n",
    "* downloard the data from data set in arff format from: https://www.openml.org/d/1085\n",
    " \n",
    "* place data in the same folder as this notebook and run the code in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALTERNATIVE HACK for loading anthracyclineTaxaneChemotherapy ## \n",
    "from scipy.io import arff\n",
    "dataset = arff.loadarff('phpCLGrjq.arff')\n",
    "import pandas as pd\n",
    "Xdf = pd.DataFrame(dataset[0])\n",
    "Xy = Xdf.as_matrix()\n",
    "n_rows, n_cols = Xy.shape\n",
    "X = Xy[:,:-1]\n",
    "X = np.float_(X)\n",
    "y = Xy[:,-1]\n",
    "y = (np.int_(y))*2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test and training. Only use 20% of data for testing because data set is small. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) \n",
    "n, d = X_train.shape\n",
    "print('{n} training data points and {d} features'.format(n = n,d =d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) fit, transform and score the knn Classifier\n",
    "n_neighbors =2 # <-- use this number of neighbours\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "trainscore = knn.score(X_train, y_train)\n",
    "training_accuracy.append(trainscore)\n",
    "testscore = knn.score(X_test, y_test)\n",
    "test_accuracy.append(knn.score(X_test, y_test))\n",
    "print (\"project dim %5d gives: (train, test) =  (%.4f, %.4f)\" % (d, trainscore,testscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "training_accuracy = []\n",
    "n_neighbors =2  #< -- I recommend 2 neighbors\n",
    "\n",
    "## Suggested range of projected dimensions:\n",
    "upperbnd = int(min(10*n,d/2))\n",
    "minbnd = int(max(n/20,d/2000))\n",
    "project_dimensions = range(minbnd,upperbnd,int((upperbnd -minbnd)/10))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    \n",
    "for r in project_dimensions:  \n",
    "    W = GenerateSparseTransform(s =20, r= r,d = d) # function coded above\n",
    "    Xt_train = ApplySparseTransform(W,X_train)  # function coded above\n",
    "    Xt_test =  ApplySparseTransform(W,X_test)  # function coded above\n",
    "    knn.fit(Xt_train,y_train)\n",
    "    trainscore = knn.score(Xt_train, y_train)\n",
    "    training_accuracy.append(trainscore)\n",
    "    testscore = knn.score(Xt_test, y_test)\n",
    "    test_accuracy.append(knn.score(Xt_test, y_test))\n",
    "    print (\"project dim %5d gives: (train, test) =  (%.4f, %.4f)\" % (r, trainscore,testscore))\n",
    "\n",
    "list_proj_dims = list(project_dimensions)\n",
    "plt.plot(list_proj_dims,training_accuracy, label='Accuracy of the training set')\n",
    "plt.plot(list_proj_dims,test_accuracy, label='Accuracy of the test set')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Projected Dimension')\n",
    "plt.legend()\n",
    "index_max = np.argmax(test_accuracy)\n",
    "print(\"Best score was for r =%5d with: (train, test) =  (%.4f, %.4f)\"% (list_proj_dims[index_max], training_accuracy[index_max],test_accuracy[index_max]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and test HARDER data set *sector.scale*  \n",
    "**(only try this data set after successfully testing the anthracyclineTaxaneChemotherapy data set)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data set from:\n",
    "# https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/sector/sector.scale.bz2\n",
    "# place data in the same folder as this python notebook\n",
    "dataname = \"sector.scale\"  \n",
    "X, y = get_data(dataname)\n",
    "n, d = X.shape\n",
    "print('{n} data points and {d} features'.format(n = n,d =d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test and training \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "n, d = X_train.shape\n",
    "print('{n} trainig data points and {d} features'.format(n = n,d =d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO ### \n",
    "## repeat the same experiments for this larger data set\n",
    "## HINT: Only test project dimensions r <= int(min(2*n,d/2))\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "training_accuracy = []\n",
    "n_neighbors =1   ## <-- I recommend using this\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    \n",
    "for r in project_dimensions:  \n",
    "    ### TODO ###   \n",
    "    # 3)    project the data matrix $X \\rightarrow XW$ using Gaussian and fit, \n",
    "    #      transform and score using knn\n",
    "    # W = GenerateSparseTransform(???)\n",
    "    # Xt_train = ApplySparseTransform(W,X_train)\n",
    "    # Xt_test =  ApplySparseTransform(W,X_test)\n",
    "    # trainscore =  ??\n",
    "    # training_accuracy.append(trainscore)\n",
    "    # testscore = ??\n",
    "    # test_accuracy.append(testscore)\n",
    "    #############\n",
    "    print (\"project dim %5d gives: (train, test) =  (%.4f, %.4f)\" % (r, trainscore,testscore))\n",
    "\n",
    "list_proj_dims = list(project_dimensions)\n",
    "plt.plot(list_proj_dims,training_accuracy, label='Accuracy of the training set')\n",
    "plt.plot(list_proj_dims,test_accuracy, label='Accuracy of the test set')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Projected Dimension')\n",
    "plt.legend()\n",
    "index_max = np.argmax(test_accuracy)\n",
    "print(\"Best score was for r =%5d with: (train, test) =  (%.4f, %.4f)\"% (list_proj_dims[index_max], training_accuracy[index_max],test_accuracy[index_max]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus question\n",
    "\n",
    "Using the Jonhson-Lindenstrauss Lemma, we can have an estimate for the projected dimension using\n",
    "\n",
    "$$r = \\frac{1}{\\epsilon^2}\\log(n/\\delta) $$\n",
    "\n",
    "1) Write a function for calculating this given suggested project dimension r. \n",
    "2) Test for each of the above data sets with \\epsilon = 0.05 = \\delta and compare to your results. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
