# Optimization for Machine Learning mini-course


Modern machine learning relies heavily on optimization tools, typically to minimize the training problem (minimizing a loss function over a training set). In this mini-course I will give a brief introduction into the training problem and motivate why the Stochastic Gradient Method (SGD) is well suited for this task. I will then present a convergence theory for SGD for convex training problems. We will then conclude with several variants and tricks used in combination with SGD such as mini-batching, averaging, and momentum (time allowing).

## Core infos

- Where    : online

- Volume   : 3h = 3 * 50min lectures + questions

- How long : 1 weeks 

- All teaching materials on:  https://gowerrobert.github.io/



## Teaching staff

R. M. Gower     <gower.robert@gmail.com>